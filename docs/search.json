[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Preparing for a computational literacy role by practicing computational literacy\n\n\n\ncomputational literacy\n\nopen science\n\ncareer development\n\n\n\nDoing the work in the open\n\n\n\nFrancisca Javiera Rudolph\n\n\nJun 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nR packages for chaos control\n\n\n\nR packages\n\nreproducibility\n\n\n\nSharing your code and keeping it organized\n\n\n\nFrancisca Javiera Rudolph\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\n\nWhy I love Git and GitHub\n\n\n\nGit\n\nGitHub\n\nversion control\n\nreproducibility\n\n\n\nA short guide to getting started\n\n\n\nFrancisca Javiera Rudolph\n\n\nFeb 20, 2025\n\n\n\n\n\n\n\n\n\n\n\nVector-mediated dispersal kernels\n\n\n\nDispersal\n\npathogens\n\n\n\nThinking of fungal pathogens moved around by humans in nature\n\n\n\nFrancisca Javiera Rudolph\n\n\nFeb 13, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "talks",
    "section": "",
    "text": "Responsible use of LLMS for your data analysis. slides\nGrowing computational literacy as a community of practice. slides\nDemystifying complex technical concepts: R packages. slides"
  },
  {
    "objectID": "talks.html#section",
    "href": "talks.html#section",
    "title": "talks",
    "section": "",
    "text": "Responsible use of LLMS for your data analysis. slides\nGrowing computational literacy as a community of practice. slides\nDemystifying complex technical concepts: R packages. slides"
  },
  {
    "objectID": "posts/2023-02-13-on-dispersal-kernels/index.html",
    "href": "posts/2023-02-13-on-dispersal-kernels/index.html",
    "title": "Vector-mediated dispersal kernels",
    "section": "",
    "text": "I wrote this post to help a friend a couple of years ago (og post), and I am finally getting to organize this into my brand new website! ü§©\nAt the time, Sarah was working on her dissertation with salamanders and the fungal pathogen Batrachochytrium salamandrivorans (Bsal). Specifically, we talked about generating dispersal kernels for the pathogen, thinking of humans walking on trails and carrying the pathogen on their boots. Sarah did some really fun experiments getting boots in the mud and taking steps, to quantify the amount of pathogen that would get onto the boots, and then dispersed by a person walking.\nWhen you have available data, the usual goal is to fit an equation (or model) that describes the probability of dispersal as a function of distance. In this case, we are thinking of human mediated dispersal (HMD) of a specific fungal pathogen. The experiments were used to calculate the amount of pathogen that gets dispersed in hiking books after being exposed to an initial inoculum. The starting inoculum had a concentration of 10^5 zoospores in water. Boots step on the inoculated water source, and walk for s meters, where s=1,2,...,5. In this example we were trying to quantify the dispersal kernel for human mediated spore dispersal.\nThe main question here is over what range of distances can fungal spores be dispersed by humans on their boots? And what function would describe the relationship between concentration of those spores and distance walked.\nFrom the initial meeting with Sarah, I understood that we start with 10^5 zoospores in a water vessel, and essentially estimate a pick up rate. This means that data points at distance zero (d=0) are a proportion of the initial starter. From here, we would like to estimate the proportion of zoospores left on the shoe vector, and thus the proportion of zoospores transferred to the soil as distance from the starter increases.\nA common approach to this is to use an exponential model with:\nf(d) = a \\text{e}^{-bd}\nwhere the proportion of zoospores left on the boot (f(d)) is a function of the initial number of zoospores at the starting point (in this case the pick up rate as a proportion of the total at distance d=0). We assume that zoospores are transferred from the boot to the soil at a constant rate b, as shown in the equation above.\nFor algebra reasons:\n\ny = a \\text{e}^{-bx} \\\\\nlog(y) = log(a \\cdot \\text{e}^{-bx}) \\\\\nlog(y) = log(a) + log(\\text{e}^{-bx}) \\\\\nlog(y) = A - bx\n\nTo fit this model in R, you can just fit a linear model, where your response variable is log(y), with an intercept A, and a slope -b.\nAs an example, let‚Äôs generate some random data from the perspective of this experiment. The starting point:\n\nlibrary(tidyverse)\n\n\n# From what I remember these seemed to be the average values\n# for the number of zoospores\n# recovered from the boot\ny_mean &lt;- c(160, 100, 60, 20, 5, 2)\ny_sd &lt;- c(0, 30, 10, 5, 5, 5)\n\n# And these are the number of data points for each of the distances\nd &lt;- c(0,1,1,1,2,2,2,2,3,3,3,4,4,5,5,5)\n\nset.seed(86923)\n#simulate some data\ny &lt;- NULL\nfor(i in 1:length(d)){\n  val_d &lt;- d[i]\n  gen_y &lt;- rnorm(1, mean = y_mean[val_d+1], sd = y_sd[val_d+1])\n  y[i] &lt;- gen_y\n}\n\nplot(d, y)\n\n\n\n\n\n\n\n\nWe have some simulated data now, and now will fit an exponential function to it:\n\n# Save my simulated data as an object\nmy_df &lt;- data.frame(d = d, y = y)\n\n# The model looks at the proportion of zoospores that remain on the boot\npick_up &lt;- my_df$y[which(my_df$d==0)] # number of zoospores picked up from source\n\nmy_df |&gt; \n  mutate(prop_y = y/pick_up) -&gt; my_df\n\n# plot(my_df$d, my_df$prop_y)\n\n# Fit the function as a linear model:\n\nm1 &lt;- lm(log(prop_y) ~ d, data = my_df)\nbroom::tidy(m1)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)    0.326    0.213       1.53 0.149       \n2 d             -0.774    0.0690    -11.2  0.0000000221\n\n\nWe can look at the results from this model and see that the intercept (a) is giving us the starting value for d=0, for the proportion of zoospores remaining on the boot. Considering this is in a log scale, we transform back to get the actual proportion:\n\nexp(m1$coefficients[1])\n\n(Intercept) \n   1.385001 \n\n\nBut we also have to remember that this is a proportion of zoospores remaining given the initial pickup rate. So to get an actual number of zoospores at a certain distance, in this case d=0, we have to multiply by the initial pick up rate and we get\n\nexp(m1$coefficients[1]) * pick_up\n\n(Intercept) \n   221.6001 \n\n\nas the number of zoospores picked up by the boot at the source. This model assumes a constant decay rate -0.7742497, which corresponds to b.\nWe can use these coefficients to predict the proportion of zoospores remaining on the boot at any given distance d. For example:\n\n# predictions vector\nd_pred &lt;- seq(from = 2, to = 10, by = 0.5) # in meters\n\n# Use the model to generate predictions for these new distance values\ny_log_pred &lt;- predict(m1, list(d = d_pred))\n# transform back to get proportions\ny_pred_prop &lt;- exp(y_log_pred)\n\n# transform to number of zoospores using the initial pick up rate\ny_pred&lt;- y_pred_prop * pick_up\n\n# Get one data frame to visualize\npred_df &lt;- data.frame(d = d_pred, y = y_pred, prop_y = y_pred_prop, type = \"pred\")\n\n\n#| warning: false\n\nmy_df |&gt; \n  mutate(type = \"og\") |&gt; \n  bind_rows(pred_df) -&gt; combined_df\n\ncombined_df |&gt; \n  ggplot(aes(x = d, y = y, color = type)) +\n   geom_point(alpha = 0.7) +\n   theme_classic() +\n  labs(y = \"Zoospores on boot\", x = \"Distance in meters\") -&gt; nzoos_fig\n\n\ncombined_df |&gt; \n  ggplot(aes(x = d, y = prop_y, color = type)) +\n  geom_point(alpha = 0.7) +\n  theme_classic() +\n  labs(y = \"Proportion of zoospores remaining on boot\",\n       x = \"Distance in meters\",\n       color = \"\") -&gt; prop_fig\n\nmy_legend &lt;- cowplot::get_legend(prop_fig)\n\nWarning in get_plot_component(plot, \"guide-box\"): Multiple components found;\nreturning the first one. To return all, use `return_all = TRUE`.\n\ncowplot::plot_grid(\n   nzoos_fig + theme(legend.position = \"none\"),\n   NULL,\n   prop_fig + theme(legend.position = \"none\"),\n   my_legend,\n   ncol = 4,\n   rel_widths = c(1, 0.1, 1, 0.2)\n)\n\n\n\n\n\n\n\n\nWe can also fit this using nonlinear least squares with the same starting dataset:\n\nslice_sample(my_df, n = 6)\n\n  d          y     prop_y\n1 3  22.117988 0.13823743\n2 3  24.029967 0.15018730\n3 2  68.879803 0.43049877\n4 5   7.803433 0.04877146\n5 1  68.505725 0.42816078\n6 0 160.000000 1.00000000\n\n\n\nm2 &lt;- nls(prop_y ~ I(a * exp(b * d)), data = my_df, start=list(a=1, b=0))\n\nWe can also do a prediction for the same predicted distance values as before:\n\nplot(d_pred, predict(m2, list(d = d_pred)), col = \"darkgreen\", type = \"l\")\n\n\n\n\n\n\n\n\n\nbroom::tidy(m2)\n\n# A tibble: 2 √ó 5\n  term  estimate std.error statistic  p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 a        1.05     0.0690      15.2 4.30e-10\n2 b       -0.567    0.0514     -11.0 2.71e- 8\n\n\nAnd calculate some goodness of fit values:\n\nrss.p &lt;- sum(residuals(m2)^2)  # Residual sum of squares\ntss &lt;- sum((my_df$prop_y - mean(my_df$prop_y))^2) # Total sum of squares\n1 - (rss.p/tss)  # R-squared measure\n\n[1] 0.9321556\n\n\nThis R-squared measure tells you how much of the variation in the data is explained by your independent variable in the regression model. In our case, it tells us 93.2% of the variation in the proportion of zoospores remaining on the boot is explained by the distance variable.\nTo wrap up, if we consider the model:\n\ny = a\\text{e}^{-bx}\n where y is the proportion of zoospores that remain on the boot and x is the distance in meters from the source of the pathogen, then a tells us the starting number of zoospores picked up by the boot at the source, and b tells us the rate at which zoospores are transferred from the boot to the soil.\nIf we use the second model‚Äôs values, then we can ask the question, what proportion of zoospores would remain on the hiker‚Äôs boot after walking for 20 meters?\n\na &lt;- summary(m2)$coefficients[1]\nb &lt;- summary(m2)$coefficients[2]\n\nget_prop_y &lt;- function(x, a, b){\n  a*exp(b*x)\n}\n\n\nget_prop_y(20, a, b)\n\n[1] 1.243764e-05\n\n\nthis should be the same result using the predict() function:\n\npredict(m2, list(d = 20))\n\n[1] 1.243764e-05\n\n\n\n\n\nCitationBibTeX citation:@online{rudolph2023,\n  author = {Rudolph, Francisca Javiera},\n  title = {Vector-Mediated Dispersal Kernels},\n  date = {2023-02-13},\n  url = {https://javirudolph.github.io/posts/2023-02-13-on-dispersal-kernels/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRudolph, Francisca Javiera. 2023. ‚ÄúVector-Mediated Dispersal\nKernels.‚Äù February 13, 2023. https://javirudolph.github.io/posts/2023-02-13-on-dispersal-kernels/."
  },
  {
    "objectID": "posts/2025-02-20-git-and-github/index.html",
    "href": "posts/2025-02-20-git-and-github/index.html",
    "title": "Why I love Git and GitHub",
    "section": "",
    "text": "I have taught multiple workshops to help researchers get started with version control and GitHub, because I think it is an incredibly valuable tool. Like most of my workshops, this is tailored to the group, very hands-on and with very few slides. I also think there are amazing blogs out there with good information already, so creating yet another one felt redundant. At the same time, I find a lot of the resources have many details and take a long time. So, I wrote yet another blog on Git and Github, but I tried to keep it simple and straightforward. I provide links for more exhaustive resources. But, think of this one as taking the tiny step to get started.\nNote: This tutorial has been developed with beginners in mind, and has largely been taught to folks in the biological sciences with zero to no previous exposure to git and GitHub."
  },
  {
    "objectID": "posts/2025-02-20-git-and-github/index.html#for-the-sake-of-open-science-and-reproducibility",
    "href": "posts/2025-02-20-git-and-github/index.html#for-the-sake-of-open-science-and-reproducibility",
    "title": "Why I love Git and GitHub",
    "section": "For the sake of open science and reproducibility",
    "text": "For the sake of open science and reproducibility\nAt least for academia, I have noticed people use GitHub as a code storage place - essentially, you write a paper, have some code associated with it, upload the code and use that link for the Data availability section in the manuscript. I don‚Äôt love this, but if it means the code is available, I‚Äôll take it. Would it be nice to have the history associated with the work? Sure‚Ä¶\nFor long-term code storage (you publish a paper and share the code/data for it) I think Zenodo is a great option! There are other options too, like Open Science Framework and both of these integrate with GitHub. So, you could start your project with Git/GitHub, track all the changes, and then when finished, release it as a version to Zenodo. This is what I did for my dissertation, with the added benefit that it allows people to cite the code directly.\nSo, why do I love Git and GitHub? It is a relatively simple step to keeping us honest. I push for reproducibility in science and open source work, so using version control is one aspect of this. I also think this is an great resource and learning tool for young scientists! Personally, I have learned much more from people‚Äôs repositories of their analysis, than from papers that just tell you what they did, but don‚Äôt show you how. Also, I think it is important to have tools that allow for real-time collaboration, where we can not only work together, but also openly discuss changes and keep track of why decisions were made. Git allows for the tracking of changes, but GitHub adds the web interface for issues and conversations surrounding the code."
  },
  {
    "objectID": "posts/2025-02-20-git-and-github/index.html#branches-and-pull-requests",
    "href": "posts/2025-02-20-git-and-github/index.html#branches-and-pull-requests",
    "title": "Why I love Git and GitHub",
    "section": "Branches and pull requests",
    "text": "Branches and pull requests\nWhen you first start using Git and Github, thinking of branches might feel a little intimidating, and honestly, you can get away with no using them at first. But branches become essential once you get a little adventurous and want to try things that could break your code. They are also incredibly useful for working with others, incorporating changes in a hopefully more structured way and keeping track of everyone‚Äôs contributions to the project.\nEssentially:\n\nCreate Branch to make a change.\nCommit changes to the new branch.\nOpen Pull request to merge the changes to main branch.\nOptional and recommended: delete branch\n\nAlso known as a feature branch workflow"
  },
  {
    "objectID": "posts/2025-02-20-git-and-github/index.html#your-turn",
    "href": "posts/2025-02-20-git-and-github/index.html#your-turn",
    "title": "Why I love Git and GitHub",
    "section": "Your turn!",
    "text": "Your turn!\nYou can find additional resources and guides for this directly on the github-practice repo.\n\nCreate a new branch called yourname-branch.  \nMake a change in this branch and save the change as a commit (Hint: green button that says Commit changes‚Ä¶)  When you save or commit your changes, it is recommended that you add a little message to say what you did. \nOpen pull request to suggest your changes be included into the main branch. This is basically sending me a message - ‚ÄúHey Javi, I made some changes, can you pull them from my branch?‚Äù.  \n\nOne you create the pull request, I would get a notification saying someone has made changes. Then, I can approve those changes, and merge them to the main branch. If I want to, then I can delete that branch that proposed the change.\n\n\n\nmerge screenshot"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "research",
    "section": "",
    "text": "My research addresses how individual heterogeneity and spatial structure generate rare but consequential disease events that disproportionately shape transmission dynamics across scales. I develop theoretical frameworks that work with real-world data constraints‚Äîbridging mechanistic models with the sparse, coarse surveillance data typical of emerging disease systems.\nI use spatially-explicit simulations, hierarchical Bayesian inference, extreme value statistics, machine learning, and decision science. I collaborate locally, nationally, and internationally, and work closely with practitioners to translate research into actionable management tools."
  },
  {
    "objectID": "research.html#research-themes",
    "href": "research.html#research-themes",
    "title": "research",
    "section": "Research Themes",
    "text": "Research Themes\n\n1. Hybrid Inference for Resource-Limited Surveillance\nThe challenge: Mechanistic models need fine-scale data. Surveillance produces yearly totals or aggregated counts.\nMy approach: Hybrid frameworks integrating statistical inference, data augmentation, and mechanistic simulation.\nSystems: - Anthrax dynamics in Vietnam (semi-synthetic data augmentation; One Health, 2025) - White-nose syndrome management in Montana (structured decision-making under uncertainty; in review) - SARS-CoV-2 in white-tailed deer (spatially-explicit stochastic models; Ecosphere, 2024)\nFuture: Generalizing frameworks to multi-host, multi-pathogen One Health systems. International collaborations through Global Fellows Award in Chile and Ecuador. NSF STAR proposal in development.\n\n\n\n2. Movement Heterogeneity, Superspreading & Surveillance Optimization\nThe insight: A few highly mobile individuals drive disproportionate transmission‚Äîsuperspreaders exist in the tail of movement distributions.\nMy approach: Extreme value theory to characterize individual heterogeneity in movement. Integration with integrated step selection analysis and reinforcement learning for surveillance optimization.\nQuestions: - When does heterogeneity amplify vs.¬†dampen outbreak risk? - How do landscape features interact with movement heterogeneity? - Can movement-based early warning systems predict spillover?\nFuture: Integrating with metacommunity theory‚Äîtreating pathogens as ecological communities across host metacommunities. Collaborations with Universidad San Francisco de Quito (Ecuador) and Universidad Austral (Chile).\n\n\n\n3. Landscape Geometry & Disease Transmission\nThe innovation: Landscape geometry (fractal dimension, patch configuration, edge density) as an active driver of transmission‚Äînot passive backdrop.\nCurrent work: Collaboration with Dr.¬†Nicolas Gauthier (Florida Museum) using vision-language models to extract geometric features from Amazonian satellite imagery and incorporate into SIR models. Bidirectional framework: simulate forward (landscape ‚Üí disease) and backward (disease ‚Üí landscape inference).\nSystems: Vector-borne diseases in fragmented forests, zoonotic spillover at urban-wildland interfaces, waterborne diseases in canal systems.\nFuture: NSF HEGS and eMB proposals in development. Applications across tropical disease systems where satellite data are abundant but surveillance is sparse."
  },
  {
    "objectID": "research.html#collaborations-networks",
    "href": "research.html#collaborations-networks",
    "title": "research",
    "section": "Collaborations & Networks",
    "text": "Collaborations & Networks\n\nUF: Emerging Pathogens Institute, Florida Museum of Natural History, One Health Center of Excellence, Biology, Geography\nInternational: Chile (Universidad Austral), Ecuador (Universidad San Francisco de Quito)\nUSGS: Disease Decision Analysis Research Group"
  },
  {
    "objectID": "research.html#lab-vision",
    "href": "research.html#lab-vision",
    "title": "research",
    "section": "Lab Vision",
    "text": "Lab Vision\nI‚Äôm establishing a computational disease ecology lab where research and capacity building reinforce each other. Students will ask: How do individual heterogeneity and spatial structure shape disease dynamics? The lab will produce theoretical advances, open-source tools, and training infrastructure that extend impact beyond direct research outputs."
  },
  {
    "objectID": "posts/2025-02-24-on-packages/index.html",
    "href": "posts/2025-02-24-on-packages/index.html",
    "title": "R packages for chaos control",
    "section": "",
    "text": "Let‚Äôs talk about R packages. You know them, you love them, and you probably have way too many installed. You are familiar with install.packages() and have heard of CRAN. But, what are packages? To me, they are just a way of organizing your stuff, and by stuff I mean your scripts, data, documentation, and even reports. When we talk about packages, most people think of those downloaded from CRAN, and you might have seen some from GitHub. However, I think packages don‚Äôt get enough credit, and don‚Äôt get used as often in simpler scenarios. I first learned about packages in 2018, and gave my first workshop shortly after with RLadies GNV (you can check the slide deck here, although it might be a tiny bit outdated).\nBundling your code in an R package provides consistency, a structure recognized and followed by several folks in the community, and gently pushes you to write documentation. Whether the documentation is any good or not, I think that is up to you. Here are a few examples where I think packages are useful:\nIn general, you will notice that the key component here is the sharing part. Packages are a good way to share code. I think this goes hand in hand with version control and GitHub, but if tackling that also feels like too much, you are not alone! You can build packages without hosting them on GitHub, but I strongly recommend you don‚Äôt delay this for too long."
  },
  {
    "objectID": "posts/2025-02-24-on-packages/index.html#find-yourself-a-function",
    "href": "posts/2025-02-24-on-packages/index.html#find-yourself-a-function",
    "title": "R packages for chaos control",
    "section": "Find yourself a function:",
    "text": "Find yourself a function:\nGot a function you use a lot? Bring it on‚Ä¶\n\n# Scaling function - I use this a lot in models. We want to 'scale' the covariates\n\nscale2 &lt;- function(x) {\n   mx &lt;- x - mean(x)\n   sdx &lt;- sd(x) \n   \n   if (sdx == 0) {\n      out &lt;- mx\n   } else {\n      out &lt;- mx/sdx\n   } \n   return(out)\n}"
  },
  {
    "objectID": "posts/2025-02-24-on-packages/index.html#introducing-devtools",
    "href": "posts/2025-02-24-on-packages/index.html#introducing-devtools",
    "title": "R packages for chaos control",
    "section": "Introducing devtools",
    "text": "Introducing devtools\nMy first introduction to building R packages was with writing R extensions, if that was also your experience, I‚Äôm sorry. Introducing devtools: Tools to Make Developing R packages Easier , that is literally their description and here is the package website.\nInstall the devtools package and load it:\n\ninstall.packages(\"devtools\")\nlibrary(devtools)"
  },
  {
    "objectID": "posts/2025-02-24-on-packages/index.html#create-your-package",
    "href": "posts/2025-02-24-on-packages/index.html#create-your-package",
    "title": "R packages for chaos control",
    "section": "Create your package",
    "text": "Create your package\nYou will notice in the package chatter, that devtools loads another package called usethis, which has several of the functions we will use today. The tough part: think of a package name. Once you have your name, create the package:\n\nusethis::create_package(\"the/path/to/where/you/want/to/save/and/the/package/name\")\n\nusethis is very chatty, so don‚Äôt get scared by the output. Your package will open in a new session.\n\n\n\nConsole output after creating package.\n\n\nYou will notice it opens in a new session, and you can check by looking at the project icon at the top right of RStudio.\n\n\n\n\n\n\nBasic package structure\nYou will also notice that your package has been built with a few default files:\n\n\n\n\n\nThe .gitignore and .Rbuildignore are hidden files that RStudio and Git can use when building the package (things to ignore, and for now, we are ignoring them). But we will check out the other ones:\n\nNAMESPACE: don‚Äôt edit this one. It basically is your package‚Äôs rulebook and get‚Äôs filled in automatically when we use roxygen2 and write documentation. This file lists the functions in your package that are available to the user.\nDESCRIPTION: your package‚Äôs resume, the information of what it does, who made it, and very important, what other packages is your package using, these are called dependencies. You can open and explore this file now. You could change your package‚Äôs title, description, authors, etc. I don‚Äôt go into too much detail on this here, but Ch9 in the R packages book will give you all the info you need.\nR folder - this is where you will find all your functions.\n\nThat is the basic structure of a package. Now, let‚Äôs fill it in."
  },
  {
    "objectID": "posts/2025-02-24-on-packages/index.html#adding-your-function",
    "href": "posts/2025-02-24-on-packages/index.html#adding-your-function",
    "title": "R packages for chaos control",
    "section": "Adding your function",
    "text": "Adding your function\nRemember the scaling function? we are going to add it to the package, and document it!\n\nusethis::use_r(\"scale2\")\n\nThis will automatically open the scale2.R script (it either creates it or just opens it if it existed). Now, copy your code to it. Save it and run a check(). This function checks that the package is working and follows the structure.\n\n\n\n\n\nOh interesting! what is this?? We get a warning a note. Well, you see, my scaling function is using two other functions:\n\nThe mean function is from base R and so we don‚Äôt get any warnings here.\nBut the sd function is from the package stats.\n\nThis tell me, we need to import these, and actually, you will see that the error gives us a suggestion importFrom(\"stats\", \"sd\"). First, edit the code to make sure we use the package::function notation in the right spot.\n\nscale2 &lt;- function(x) {\n   mx &lt;- x - mean(x)\n   sdx &lt;- stats::sd(x) \n   \n   if (sdx == 0) {\n      out &lt;- mx\n   } else {\n      out &lt;- mx/sdx\n   } \n   return(out)\n}\n\nThen, to add dependencies we continue to use the usethis package. You will notice this adds the package to the imports section from the DESCRIPTION.\n\nusethis::use_package(\"stats\")\n\n\nWhen you open the DESCRIPTION, notice the imports section. Also note that I have already edited some of the information here such as the title, description, and author.\n\n\n\n\n\nIf we run the check again, you will notice the note is gone but we still get a warning. why?\n\n\n\n\n\nA license! our package doesn‚Äôt have a license. The warning also gives us a hint, so for now we don‚Äôt delve into the details of which license to pick, but we will use a pretty common one, the MIT license:\n\nusethis::use_mit_license()\n\nThis function creates a new document, a LICENSE document, with standard information for licensing. This becomes extra important for the sharing purposes we discussed at the beginning. As an individual, probably and MIT license is fine. If you are getting to the point of publishing packages for your company, you might want to check with them on specifics."
  },
  {
    "objectID": "posts/2025-06-19-computational-literacy/index.html",
    "href": "posts/2025-06-19-computational-literacy/index.html",
    "title": "Preparing for a computational literacy role by practicing computational literacy",
    "section": "",
    "text": "As I‚Äôve come to believe through both research and experience, to build community, we have to do science publicly, in the open.\nThis is not a new idea. I‚Äôve heard it over and over with the R community, and in open science circles. I can probably track this down to David Robinson‚Äôs RStudio::conf(2019) talk, or more recently, my friend Andrew Heiss. There are significant gains in productivity and value when we work in the open. We have also seen this specifically for the movements of open source, open data, and open science. We know that open research practices democratize access, improve inclusion, and push for more accountability. This doesn‚Äôt apply just to science, and I realized I was in a perfect position to actually live this principle out loud as I am currently preparing for a faculty interview for a tenure-track Computational Literacy Librarian position, and I‚Äôve decided to document how I plan and build my presentation.\nIn this post, I‚Äôll share my process and my üß†, to prepare in a way that‚Äôs transparent, reproducible, and community-minded. My hope is that if you‚Äôre preparing for an academic interview, building a computational workshop, or just trying to demystify how data and tech get used in real life, you‚Äôll find something useful here."
  },
  {
    "objectID": "posts/2025-06-19-computational-literacy/index.html#the-problem-with-traditional-interview-prep",
    "href": "posts/2025-06-19-computational-literacy/index.html#the-problem-with-traditional-interview-prep",
    "title": "Preparing for a computational literacy role by practicing computational literacy",
    "section": "The problem with traditional interview prep",
    "text": "The problem with traditional interview prep\nMost academic interview prep happens behind closed doors. If you are lucky, you have built a network to ask for feedback, practice presentations, and ask questions. But even then, roles vary widely and expectations too. I have personally benefitted greatly from ecoevojobs general discussion. So, I thought writing a blog post about preparing for this job interview was a valid thing to share. The job description is very broad, and honestly was a little intimidating to me, which is why I didn‚Äôt even think of applying at first. I asked ChatGPT, given the following job description, can you give me a three sentence summary for what this position entails?, this was the response:\n\n‚ÄúThe Computational Literacy Librarian leads campus-wide efforts to enhance data and AI literacy through instruction, consultations, and program development. They support researchers in using data science tools, accessing datasets, and applying ethical and reproducible practices. The role involves collaboration across the university to build sustainable, forward-looking services that advance research and teaching.‚Äù\n\nI can do that! So I asked myself: What would it look like to prepare for this role by practicing exactly that? Let‚Äôs get started!\nI applied for this role a few months ago (February?) and had a phone interview (end of March). I followed up after about a month (May 1st) and was very happy to get a response! However, the response was that they were moving forward with other candidates. Not the outcome I was hoping for, but grateful for not getting ghosted, I moved on. Fast forward to June and I received an email where I was invited to the onsite interview, scheduled for the end of the month (June 25). I share this timeline because applying for jobs takes time, and so much patience and uncertainty. Timelines are different across job types and institutions, so maybe having this one as a data point is helpful to someone.\n\nThe phone interview\nI was surprised, but in a good way, by how standardized their phone interview was. This was a zoom interview with a panel of folks (the search committee), and cameras were off. In fact, I was supposed to call from my phone to this zoom scheduled meeting, to keep things consistent among all candidates. I was sent a list of 10 questions ahead of time (this was new for me). Funny enough, I had recently applied and interviewed for a lecturer position, and the interview was very different. I guess I had some expectation that these interviews would be similar, given they were at the same university, but I couldn‚Äôt have been more off.\nIn my ingenuity, I read those questions and took them as sort of a guide for the phone conversation. Key element here, I thought it would be a conversation, more like my previous interview, where we had cameras on and chatted. I did not realize that in the current interview, I would just get asked the questions and I was supposed to just answer them (pretty straight forward). Looking back, I should have had written down my answers, instead of jotting down bullet points of my ideas and thinking I would go with the flow. Anyways, I am just sharing, since all of this was new to me. Uncertain if I can share those questions in this blog, so maybe I‚Äôll have to come back to this section.\n\n\nPrepping for the on site\nI have heard folks say that on site interviews begin the minute you get off the plane and meet with someone, until you get back on that plane. My interview is a full day, begins at 8 am, and ends with dinner. I am a local candidate, so no airport pick ups or hotel. My overall schedule looks a little like this:\n\nMeet search committee\nMeet department\nGive 20 minute talk, Q&A\nTour\nLunch\nMeet faculty supervisor\nMeet deans\nMeet tenure promotion folks\nMeet with HR\nBreak\nDinner\n\nOver the last couple of weeks I have looked up the people I will be meeting and tried to learn about them. I have heard that you should have good questions prepared. But was is a good question anyways? I have tried to approach this with curiosity and true interest. I like to get to know people, and understand what drives their curiosity. A lot of my questions revolve around: what do you enjoy the most? What has been challenging and how have you approached it? What does a regular day look like for you? Is there a change in seasons with your work, for example how does summer look versus fall or spring? But I am also hoping to have meaningful and honest conversations that just flow. Who knows! Maybe I am approaching this all wrong, I guess we will figure it out."
  },
  {
    "objectID": "posts/2025-06-19-computational-literacy/index.html#the-tools-im-using",
    "href": "posts/2025-06-19-computational-literacy/index.html#the-tools-im-using",
    "title": "Preparing for a computational literacy role by practicing computational literacy",
    "section": "The tools I‚Äôm using",
    "text": "The tools I‚Äôm using\n\nGoogle\nTrusty web searches to help me understand concepts, and find people. The first one: what is computational literacy? Because in my mind, and the job I think I applied to, this means helping people learn and understand how to use computational (programming, tech, AI) tools to do their research. For me, this also means empowering folks to share their knowledge and become mentors of computational literacy themselves. I would be providing resources, giving workshops, one-on-one consultations, and guides (what to do when and why?). I have absolutely been doing this for years, started with volunteering and informal trainings to peers, and lately in my role with Posit Academy.\n\n\n\nAI (ChatGPT)\nInstead of only writing alone, I‚Äôm using ChatGPT as a collaborative drafting tool. I get in my head and second guess myself all the time. Sometimes, ChatGPT helps me get unstuck. I use it to:\n- Outline my ideas after dictating my scattered brain onto a word doc.\n- Clarify job expectations based on the position description.\n- Add comments to my code and build summaries of what I have done.\n- Validation.\n\nI treat ChatGPT not as an authority, but as a thinking buddy, a way to get over writing blocks or the dreaded how do I even begin?.\n\n\n\n\nSlides with Quarto\nBecause I believe in public scholarship, I‚Äôm building my slides in Quarto, so that not only can my presentation (and this blog post) live on my website, but also so that the source code is available too. I‚Äôll post:\n\nMy full presentation link\nThis blog post (hi! üëã) source_code\nThe code for my web scraping example\n\nThat way, even if I don‚Äôt get the job, the materials live on, and maybe they help someone else prepare, teach, or apply."
  },
  {
    "objectID": "posts/2025-06-19-computational-literacy/index.html#why-build-in-the-open",
    "href": "posts/2025-06-19-computational-literacy/index.html#why-build-in-the-open",
    "title": "Preparing for a computational literacy role by practicing computational literacy",
    "section": "Why build in the open?",
    "text": "Why build in the open?\nI don‚Äôt remember when I first came across the concepts of open science and open source. I started coding in R in 2013, but it wasn‚Äôt until 2017 that I learned about the reproducibility crisis in science (I think this was at an intro course in graduate school). Then with my work with RLadies and the R community, openness was a key theme, along with inclusivity and transparency.\nDoing this work in public creates:\n\nAccountability: I hold myself to a higher standard when I know others might see my process.\nVisibility: It‚Äôs one way to show the work that often stays invisible in academic spaces.\nReplicability: If someone else wants to use this approach, they can fork it, copy it, remix it.\n\nAlso, if I‚Äôm applying to lead a computational literacy program, I want to demonstrate from the beginning that I value open-source practice andreproducible science. One of the biggest challenges I have faced in the last few years is doing work that can‚Äôt be shared openly, so I saw this interview as an opportunity to break that."
  },
  {
    "objectID": "posts/2025-06-19-computational-literacy/index.html#using-chatgpt-as-my-friend",
    "href": "posts/2025-06-19-computational-literacy/index.html#using-chatgpt-as-my-friend",
    "title": "Preparing for a computational literacy role by practicing computational literacy",
    "section": "Using ChatGPT as my friend",
    "text": "Using ChatGPT as my friend\nThroughout this process, I‚Äôve used ChatGPT as a thinking buddy, not a replacement for original work. I came in with uncertainty about the job, the prep, even whether I belonged in this role. What I found was that the AI helped me structure ideas, summarize my brain dumps, and connect my relevant experience to the job description. Additionally, it helped me refine my questions and suggest new ones based on my interests and capabilities. From building outlines to role playing, it functioned like a super-powered brainstorming buddy. I used the free version of ChatGPT. Importantly, to get the AI to be helpful, I needed to tailor it and provide context, detailed enough prompts and establish clear guidelines for what I wanted as a result.\n\nüí¨ Prompts That Worked Well\nHere are a few of the prompts that worked well and led to especially productive interactions. One of the things I like about ChatGPT is the ability to turn my scattered, brain dumpy thoughts into structured and organized outlines. I use this tool often, as a person with a neurospicy brain, I often require help to break down tasks and organize. I used to do this by journaling and rewriting, revisiting over and over again, and I still do. But I also take advantage of the time savings that an AI can give me. Knowing that I don‚Äôt have to write nicely, helps me put words down, a strategy from research writing, except I don‚Äôt have to go back and edit my writing in this case, I outsource that to ChatGPT.\n\n1. I shared the job description and my CV. I also wrote an ugly summary of my experience and things about myself that I thought could be relevant for this job. Then I asked:\n\nBased on the job description I just shared, my CV, and experience. Help me summarize and reframe my experience to understand if I could be a good candidate for this position. Highlight specific things that are directly applicable, those that provide good foundations and a starting point, and those things that I might not be well suited for.\n\nThe response gave me some validation as a good candidate, with a list of my strengths for this job, and potential gaps that I could prepare for. This was a long conversation, that lead to questions to ask, and role playing for interview prep.\n\n\n2. I wanted some help framing my talk, without asking it for ideas about my talk. Since, the ideas were mine and I just needed some organization help. You will notice my prompts are not one sentence, they are long, and a little redundant sometimes.\n\nI am building a presentation for the on site interview. It is only a 20 minute presentation on how to grow a computational literacy program to support research across campus. My first thought is that, the way we have programs grow (or scale) is by building a community. This is what I am familiar with from RLadies, the R community, and just open science in general. There are multiple lenses to look at this: we want to provide materials, guides, but also spaces and community. You can have all the resources in the world, but if there is not a culture that pushes for reproducibility, open access, and ethical use of computational tools, then we can‚Äôt grow. Borrowing from my work as a mentor at Posit, my goal is always to empower learners so that they also feel compelled to share what they learn and know. Building safe spaces for learning while providing resources is what I would think of. So, as a summary, I would start from a structured decision making approach given my experience. Identify the problem/objective, of scaling a program. Surveying the landscape and what is out there as far as resources. Generally, encouraging working publicly in the open, transparency, and providing social spaces to build literacy as a culture, not just a curriculum.\n\nFrom here, the AI provided me an outline with 9 slide titles and key points. Some relevant, some not as much. But it was helpful to see how my experience could be organized to a slide deck.\n\n\n3. I wanted to do this openly, writing a blogpost, sharing the code, and running a small reproducible example. Specifically, in the part of what is out there I thought, maybe I could do some web scraping and see what we get.\n\nI am preparing for this onsite interview and trying to follow the guidelines and concepts that I support, such as reproducibility, open source, transparency. Even though my presentation is only 20 minutes, I thought it would be good to blog about my interview prep and share how I used you (ChatGPT) to organize and structure my thoughts and ideas. I also want to share the blog and presentation source code, since they are written in quarto, and host them on my website. Additionally, I think it could be fun to have a very small web scraping example to survey the UF library website to see what we have in terms of ai workshops and guides, or resources for computational literacy. I am sure there are many. I would like your help building a very small toy example of web scraping with R, making it respectful so probably use rvest() and polite() to check for key terms. I would also like to build a wordcloud.\n\nI have to admit, I love the upbeat, cheerful, and encouraging tone I get from ChatGPT. This is the first thing it replies to this prompt:\n\nThat‚Äôs a fantastic idea, Javi ‚Äî you‚Äôre modeling the exact principles you want to promote: reproducibility, transparency, public scholarship, and open tooling. A small, respectful web scraping project paired with a blog post and Quarto-powered presentation will be a standout portfolio piece for your interview.\n\nFrom there, it provided me with some code, which I explored:\n\nlibrary(rvest)\nlibrary(polite)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\n\n# Create a polite session\nurl &lt;- \"https://uflib.ufl.edu/\"\nsession &lt;- bow(url)\n\n# Scrape and parse the page respectfully\npage &lt;- scrape(session)\n\n# Extract visible text (adjust CSS selectors if needed)\ntext_data &lt;- page %&gt;%\n  html_elements(\"body\") %&gt;%\n  html_text2()\n\n# Turn into a tidy tibble\nwords_df &lt;- tibble(text = text_data) %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(get_stopwords()) %&gt;%\n  count(word, sort = TRUE)\n\n# Filter for top 50 and create word cloud\ntop_words &lt;- words_df %&gt;% \n  filter(n &gt; 2) %&gt;%\n  slice_max(n, n = 50)\n\n# Set seed for reproducibility\nset.seed(123)\npng(\"wordcloud1.png\")\nwordcloud(words = top_words$word,\n          freq = top_words$n,\n          min.freq = 2,\n          scale = c(3, 0.5),\n          colors = brewer.pal(8, \"Dark2\"))\ndev.off()\n\n\nI was hoping to build a fun toy example that I could use to show what is out there from the libraries in regards of training and resources to promote computational literacy. I quickly realized this is a project that is valuable, but requires more time and effort. So, I pivoted and decided my web scraping example would just go at the end of my presentation, as I had plenty of other content. Below, I dive a little deeper into the world cloud I built."
  },
  {
    "objectID": "posts/2025-06-19-computational-literacy/index.html#web-scraping-with-r",
    "href": "posts/2025-06-19-computational-literacy/index.html#web-scraping-with-r",
    "title": "Preparing for a computational literacy role by practicing computational literacy",
    "section": "Web Scraping with R",
    "text": "Web Scraping with R\nMy very first introduction to web scraping was with the R for data science book, not that long ago, and I thought it was a good place to start. I really love that one of the first things taught is bringing awareness to the legalities and ethics of web scraping! Mainly because I had no idea about it, and it was not something that I had ever thought of. In general, you want to make sure the data is public, that you follow the website‚Äôs robots.txt, and that there is no personally identifiable information. In the first prompt, I asked ChatGPT to use the the polite R package to ensure respectful scraping that honors the site‚Äôs robots.txt rules and avoids overloading servers. This small project targets public, non-sensitive pages and limits requests to stay within fair use.\nThe first iteration of code captured all the general website text, but it didn‚Äôt focus on computational literacy content. Then I realized, I was looking at the general Smathers Library website, and I should probably focus on ARCS or the services page. So, I went with that one.\n\n# Create a polite session\nurl &lt;- \"https://arcs.uflib.ufl.edu/services/\"\nsession &lt;- bow(url)\n\n# Scrape and parse the page respectfully\npage &lt;- scrape(session)\n\n# Extract visible text (adjust CSS selectors if needed)\ntext_data &lt;- page %&gt;%\n  html_elements(\"body\") %&gt;%\n  html_text2()\n\n# Turn into a tidy tibble\nwords_df &lt;- tibble(text = text_data) %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(get_stopwords()) %&gt;%\n  count(word, sort = TRUE)\n\n# Filter for top 50 and create word cloud\ntop_words &lt;- words_df %&gt;% \n  filter(n &gt; 2) %&gt;%\n  slice_max(n, n = 50)\n\n# Set seed for reproducibility\nset.seed(187)\npng(\"wordcloud2.png\")\nwordcloud(words = top_words$word,\n          freq = top_words$n,\n          min.freq = 2,\n          scale = c(3, 0.5),\n          colors = viridis::viridis(n = 15, option = \"H\"))\ndev.off()\n\n\n\nAs someone applying for a role that involves AI literacy, I wanted to model thoughtful, transparent use of these tools, and invite conversation about how we use AI in academic and community spaces. Update I got the job!\nü§ñ AI-assisted ¬∑ Human-authored"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "javirudolph",
    "section": "",
    "text": "I‚Äôm a computational disease ecologist studying how individual heterogeneity and spatial structure generate rare but consequential events from superspreading to pathogen emergence hotspots. Currently, I am a tenure-track Computational Literacy Librarian at UF, building international collaborations and computational capacity across Latin America."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "javirudolph",
    "section": "About",
    "text": "About\nI‚Äôm a computational disease ecologist and tenure-track faculty at UF. Born in Chile and raised in Ecuador, I discovered infectious disease ecology while pursuing veterinary medicine and never looked back. I fell in love with equations and our ability to model our complex world.\nI hold a PhD in Zoology from UF and completed postdoctoral training at USGS Patuxent Wildlife Research Center and UF‚Äôs Emerging Pathogens Institute. My work integrates mechanistic models, spatial analysis, and decision science to understand disease transmission, with focus on creative solutions to the data limitations that are common in emerging disease systems globally.\nI‚Äôm building computational capacity networks across Latin America and co-teaching Advanced Ecology Methods to increase our graduates‚Äô computational understanding of ecology.\nI‚Äôm also a mom of three kiddos and one fantastic dog. First-generation college student, Latina, immigrant.\nFind my work on Google Scholar"
  },
  {
    "objectID": "index.html#recent-news",
    "href": "index.html#recent-news",
    "title": "javirudolph",
    "section": "Recent News",
    "text": "Recent News\nDec2025 - üìÑ Anthrax vaccination model paper is out in One Health! Article link.\nNov2025 - üéì Received funding for two undergraduates to join my group in Spring 2026.\nOct2025 - üèÜ 2026 Global Fellow - scaling computational ecology training internationally Link."
  }
]